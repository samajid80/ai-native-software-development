---
description: Intelligence-driven workflow for creating book chapters (any programming language). Reads constitution + chapter-index to derive audience/complexity/prerequisites automatically. Loads official language documentation via MCP. Integrates AI-Native CoLearning pedagogy throughout lessons. Chains /sp.specify ‚Üí /sp.plan ‚Üí /sp.tasks ‚Üí /sp.implement with validation gates.
---

# /sp.chapter-writer: Universal Intelligence-Driven Chapter Workflow

**Purpose**: Design a complete book chapter (any programming language or conceptual topic) using **vertical intelligence** (constitution + chapter-index + skills + MCP documentation) to automatically derive context. Language-agnostic with AI-Native CoLearning pedagogy built-in.

**Intelligence Sources**:
- Constitution: Target audience, philosophy, learning patterns
- Chapter Index: Exact title (THE ANCHOR), part, prerequisites
- Skills Library: Available domain skills (especially `ai-collaborate-teaching`)
- MCP Servers: Official language/framework documentation (Python.org, TypeScript docs, etc.)
- Context Materials: Existing pedagogical patterns (if available)

**Adaptive Questions**: 0-3 targeted questions based on what intelligence can't derive (NOT hardcoded).

## User Input

```text
$ARGUMENTS
```

**Expected Format**:
```bash
/sp.chapter-writer <chapter-number> [--language <lang>] [--dry-run]
```

**Examples**:
```bash
/sp.chapter-writer 15 --language python       # Python chapter
/sp.chapter-writer 38 --language typescript   # TypeScript chapter
/sp.chapter-writer 5 --language conceptual    # Conceptual/narrative chapter
/sp.chapter-writer 20 --dry-run               # Plan only, no implementation
```

**Parameters**:
- `<chapter-number>`: Required. Chapter number (1-56)
- `--language <lang>`: Optional. Programming language (python, typescript, conceptual). Auto-detected from chapter-index if not provided.
- `--dry-run`: Optional. Generate spec/plan/tasks only, skip implementation.

---

## VERTICAL INTELLIGENCE: AI-Native CoLearning Teaching

Before orchestration begins, understand what makes chapters effective in the AI-native era:

### Core Principle: CoLearning WITH AI (Not Just Using AI)

Students don't just learn programming syntax. They learn to **co-reason with AI**:

1. **Understand the concept** (book teaches foundational, stable concepts directly)
2. **Explore with AI** (AI companion handles complex execution, student directs)
3. **Validate outputs** (critical thinking, never blind trust)
4. **Orchestrate at scale** (AI automates 10+ operations, student supervises)

### AI-Native CoLearning Pedagogy (Constitution Principle 13 + ai-collaborate-teaching skill)

**Traditional Programming Teaching**:
- "Memorize syntax"
- "Here are all 47 string methods"
- Syntax-first (memorize, then apply)

**AI-Native CoLearning Pattern**:
- **Book Teaches Foundational** (Tier 1): Stable concepts explained directly
- **AI Companion Handles Complex** (Tier 2): Student specifies, AI executes, student observes
- **AI Orchestration at Scale** (Tier 3): Student orchestrates, AI manages, student validates

**CoLearning Structural Elements** (throughout lessons, NOT just end):

#### üí¨ AI Colearning Prompt (Claude Code or Gemini CLI)
- Positioned after introducing foundational concepts
- Encourages students to explore deeper with AI as co-reasoning partner
- Example: "Explain how `for` loops work under the hood with `range()`."
- **Purpose**: Position AI as intellectual partner for conceptual exploration

#### üéì Instructor Commentary: "From Syntax to Semantics"
- Emphasizes conceptual understanding over syntax memorization
- Key mantra: "Syntax is cheap ‚Äî semantics is gold"
- Example: "In AI-driven development, you don't memorize operator precedence‚Äîyou understand when to ask AI for clarification."
- **Purpose**: Reframe learning goals (understanding > memorization)

#### üöÄ CoLearning Challenge
- Prompts that teach conceptual translation (intent ‚Üí AI prompt ‚Üí code ‚Üí understanding)
- Example: "Ask your AI: Generate a factorial function using recursion, then explain how recursion works step-by-step including the call stack."
- **Purpose**: Practice specification-driven thinking WITH AI collaboration

#### ‚ú® Teaching Tip
- How to use Claude Code/Gemini CLI as pair-teacher
- Example: "Use Claude Code to explore edge cases: 'What happens if I use // with negative numbers?'"
- **Purpose**: Build AI tool literacy and effective collaboration patterns

**Critical Distinctions**:
- **CoLearning Elements** (throughout lesson): Conversational, exploration-focused, AI partnership throughout content
- **Try With AI Section** (end of lesson): Structured 4-prompt synthesis and reflection (closure point)

**Tone Requirements for ALL Lessons**:
- ‚úÖ Conversational (you, your, we)
- ‚úÖ Exploration-focused (discover, explore, try)
- ‚úÖ AI partnership emphasized (co-teacher, co-reasoning partner, pair-teacher)
- ‚ùå NOT documentation style
- ‚ùå NOT reference manual tone
- ‚ùå NOT traditional tutorial "here's how you do X" without AI collaboration context

---

## Language-Specific Standards

### Python Chapters (12-29, 41-45)

**Version:** 3.14+ (always use latest stable from https://www.python.org/downloads/)
**Syntax:** f-strings only, match/case, modern types (`list[int]`, `X | None`)
**Type hints:** Mandatory from Chapter 14 onwards
**MCP Documentation Source**: Python.org official docs via context7 MCP server

**Security (non-negotiable)**:
- ‚ùå No `eval()`, `shell=True`, hardcoded secrets
- ‚úÖ Environment variables, input validation, modern patterns

**Language-Appropriate CoLearning**:
- "Ask your AI: Why does Python use `:` after `if` statements instead of `{}`?"
- "Tell your AI: Generate a function that validates email addresses and explain the regex pattern."

---

### TypeScript Chapters (38-40, 46-48)

**Version:** 5.3+ with ES2024 target
**Syntax:** Modern ES features, async/await, strict mode
**Type hints:** Mandatory throughout (TypeScript's core value)
**MCP Documentation Source**: TypeScript official docs + MDN via context7 MCP server

**Security (non-negotiable)**:
- ‚ùå No `eval()`, `innerHTML` without sanitization, hardcoded API keys
- ‚úÖ Environment variables, CSP headers, modern security patterns

**Language-Appropriate CoLearning**:
- "Ask your AI: Why does TypeScript require explicit `async` keyword when JavaScript doesn't?"
- "Tell your AI: Generate a React component with TypeScript and explain the type inference."

---

### Conceptual Chapters (1-11, 30-37, 49-56)

**Focus:** Mindset, methodology, specification-driven development, architecture
**No code requirements** (or minimal illustrative code)
**MCP Documentation Source**: Not applicable (no language docs needed)

**CoLearning Adaptation**:
- "Ask your AI: What are the ethical implications of AI-generated code in production systems?"
- "Tell your AI: Analyze this specification for ambiguities and suggest clarifications."

---

## CRITICAL DESIGN RULES

### Rule 1: USER INTENT IS AUTHORITY

**Never override user input:**
- User says "beginner" ‚Üí Make A1-A2 (NOT A2-B1)
- User says "just variables" ‚Üí Only variables (NOT + functions + loops)
- User says "absolute beginners" ‚Üí 5 concepts max, simple framing

**Always ask, always honor. Do NOT assume.**

---

### Rule 2: NO FORWARD REFERENCES + PART-APPROPRIATE LANGUAGE

**Never mention untaught concepts:**
- ‚ùå NO references to chapters not yet encountered
- ‚ùå NO professional terminology before it's taught
- ‚ùå NO "you'll learn this in Chapter N" without context

**DO reference what students already know:**
- ‚úÖ "Building on the AIDD principles from Part 1..."
- ‚úÖ "Remember validation-first from Chapter 4..."
- ‚úÖ "Using your AI as co-reasoning partner (as you learned in Part 2)..."

**Part-Specific Language**:
- **Parts 1-3**: "Describe your intent", "Ask your AI", "Explore with your AI companion"
- **Parts 4-5**: "Write clear specifications", "Validate with type hints", "Test your understanding"
- **Parts 6-13**: "Design agent architecture", "Define specifications", "Deploy to production"

---

### Rule 3: RUTHLESS CONTEXT FILTERING

**When extracting from context materials:**

**Decision Rule**:
- IF context concept fits THIS chapter's title ‚Üí EXTRACT
- IF context concept belongs to later chapters ‚Üí SKIP
- IF context concept is advanced variation ‚Üí SKIP
- IF context concept requires future prerequisites ‚Üí SKIP

**Example (Chapter 15 "Operators")**:
- ‚úÖ Arithmetic operators (+, -, *, /) ‚Üí USE (chapter focus)
- ‚úÖ Comparison operators (==, !=, >, <) ‚Üí USE (chapter focus)
- ‚ùå List comprehensions ‚Üí SKIP (Ch 18-19 topic)
- ‚ùå Exception handling ‚Üí SKIP (Ch 21 topic)

---

### Rule 4: GRADUATED COMPLEXITY (Constitution Principle 12)

**Depth > breadth.**

- **Beginner (Ch 1-16)**: 5 concepts max per lesson, 3-4 lessons per chapter
- **Intermediate (Ch 17-29)**: 7 concepts max per lesson, 4-5 lessons per chapter
- **Advanced (Ch 30-48)**: 10 concepts max per lesson, 5-6 lessons per chapter
- **Professional (Ch 49-56)**: No artificial limits, real-world complexity

---

### Rule 5: MINIMAL FILES

**Create ONLY:**
- ‚úÖ spec.md (what students learn)
- ‚úÖ plan.md (how we teach it + skills proficiency mapping)
- ‚úÖ tasks.md (implementation checklist)

**Never create:**
- ‚ùå index.md, _templates/, _assets/, _code-examples/, lesson-template.md, capstone-rubric.md

---

### Rule 6: COLEARNING IS AI PARTNERSHIP (Not Troubleshooting Appendix)

**Real-world context:** Students will encounter errors, confusion, edge cases. Rather than appendices, teach students to ASK their AI co-teacher in the moment.

**Application in chapters:**
- **Foundational chapters**: "Ask your AI: What does this error mean?"
- **Applied chapters**: "Tell your AI: I'm getting TypeError. Walk me through what went wrong."
- **Advanced chapters**: "Ask your AI: Compare these three architectural approaches and explain tradeoffs."

**Why this works:**
- ‚úÖ Teaches resilience: Errors are information, not obstacles
- ‚úÖ Builds partnership: AI becomes problem-solving collaborator
- ‚úÖ Scales with complexity: Works for simple errors to architectural decisions
- ‚úÖ Honors reality: Professional developers ask AI constantly

**Example Integration (throughout lesson, not appendix)**:
```markdown
### Understanding Comparison Operators

Comparison operators return `True` or `False`:

\`\`\`python
x: int = 10
y: int = 5

print(x > y)   # True
print(x == y)  # False
\`\`\`

#### üí¨ AI Colearning Prompt
> "Explain why `==` checks value equality but not type equality. What happens if I compare `5 == 5.0`?"

#### üéì Instructor Commentary
> In AI-native development, you don't memorize comparison rules‚Äîyou understand WHEN equality matters and ask AI to clarify edge cases. Syntax is cheap; understanding is gold.
```

---

### Rule 7: GRADUATED TEACHING PATTERN (Constitution Principle 13)

**Apply the three-tier teaching approach from constitution:**

**Tier 1 - Foundational Concepts** (Book Teaches Directly):
- Stable, core concepts explained directly in book
- Direct explanation with analogies and examples
- Examples: What are variables? What is a loop? What are type hints?
- **NO "Ask your AI: What is X?" for foundations**
- Book provides clear, authoritative explanation first
- **Then CoLearning for deeper exploration**: "Ask your AI: How does Python store variables in memory?"

**Tier 2 - Complex Syntax** (AI Companion Handles):
- Complex syntax patterns AI handles (student directs, AI executes)
- Student specifies WHAT they want, AI handles HOW
- Examples: Decorators, context managers, complex regex, async/await patterns
- "Tell your AI: Create a decorator that logs function execution time."
- Student learns strategy and intent, not memorization

**Tier 3 - Scaling Operations** (AI Orchestration):
- Operations involving 10+ items or multi-file workflows
- Student orchestrates strategy, AI manages tactical execution
- Examples: Setting up 10 test environments, batch conversions, project scaffolding
- "Tell your AI: Set up 10 worktrees for features 1-10."
- Student learns orchestration and supervision skills

**Application**:
- Primarily Tier 1+2 for beginner/intermediate chapters
- Tier 3 introduced in advanced/professional chapters
- Balance: Book teaches, AI handles complexity, student directs

---

### Rule 8: STANDARDIZED "TRY WITH AI" FORMAT (End-of-Lesson Closure)

**Every lesson MUST end with "Try With AI" section ONLY** (no summaries, no "what's next", no checklists):

```markdown
## Try With AI

Use your AI companion (Claude Code or Gemini CLI). [Brief context about what you're exploring].

### Prompt 1: [Descriptive Title - Recall/Understand]
\`\`\`
[Clear, concrete prompt asking about the concept]
\`\`\`

**Expected outcome**: [What student should understand after AI response]

### Prompt 2: [Descriptive Title - Apply]
\`\`\`
[Clear, concrete prompt about application or edge case]
\`\`\`

**Expected outcome**: [What student learns from this]

### Prompt 3: [Descriptive Title - Analyze/Evaluate]
\`\`\`
[Prompt encouraging deeper understanding or real-world connection]
\`\`\`

**Expected outcome**: [Connection to professional practice]

### Prompt 4: [Descriptive Title - Synthesis/Create]
\`\`\`
[Synthesis prompt pulling together concepts from lesson]
\`\`\`

**Expected outcome**: [Integration of understanding + forward-looking insight]
```

**Critical requirements:**
- ‚úÖ Exactly 4 prompts per lesson (progressive Bloom's levels: Remember ‚Üí Understand ‚Üí Apply ‚Üí Analyze/Synthesize)
- ‚úÖ Prompts are CONCRETE and SPECIFIC (not "ask AI about X")
- ‚úÖ Each prompt has explicit "Expected outcome"
- ‚úÖ Prompts include validation thinking ("Does this match the spec?", "What assumptions did I make?")
- ‚úÖ **NOTHING after "Try With AI"** (this IS the closure)

**CRITICAL LESSON CLOSURE PATTERN** (Constitutional Mandate):

Lessons MUST end with "Try With AI" section ONLY. Prompt 4 provides cognitive closure.

**NEVER ADD after "Try With AI":**
- ‚ùå "Key Takeaways" or "Summary"
- ‚ùå "What's Next"
- ‚ùå "Completion Checklist" (even for capstone lessons)
- ‚ùå "Chapter Summary"
- ‚ùå Any other closure content

**WHY**: Try With AI Prompt 4 already provides reflection and synthesis. Additional sections create cognitive overload and violate Constitution Principle 12 (Cognitive Load).

---

## MCP DOCUMENTATION LOADING (Phase 0 Enhancement)

**Purpose**: Load official, current documentation for programming languages/frameworks to ensure accuracy and reduce manual context preparation.

**MCP Server**: context7 (or equivalent documentation server)

**Supported Languages**:
- **Python**: Python.org official docs (current stable version)
- **TypeScript**: TypeScript official docs + MDN Web Docs
- **Conceptual**: N/A (no language docs needed)

**Loading Strategy** (Phase 0: Intelligent Context Discovery):

```python
# Pseudo-code for MCP documentation loading
if language == "python":
    docs = mcp_fetch(
        server="context7",
        resource="python-docs",
        version="3.14",  # Or latest stable
        sections=["tutorial", "library/stdtypes", "library/functions"]
    )
    context["language_docs"] = docs
    context["doc_source"] = "Python.org Official Documentation v3.14"

elif language == "typescript":
    docs = mcp_fetch(
        server="context7",
        resource="typescript-docs",
        version="5.3",  # Or latest stable
        sections=["handbook", "reference"]
    )
    context["language_docs"] = docs
    context["doc_source"] = "TypeScript Official Handbook v5.3"

elif language == "conceptual":
    # No language docs needed
    context["language_docs"] = None
    context["doc_source"] = "N/A (conceptual chapter)"
```

**Documentation Usage**:
- **During /sp.specify**: Reference docs for accurate requirement definition
- **During /sp.plan**: Use docs to validate code examples and teaching approach
- **During /sp.implement**: lesson-writer accesses docs for technical accuracy

**Fallback Strategy** (if MCP unavailable):
- Warn user: "MCP documentation server unavailable. Using cached context materials."
- Proceed with existing context materials from `context/` directory
- Flag for manual verification after generation

**MCP Invocation Pattern**:
```bash
# Example: Check if MCP context7 server is available
if mcp_server_available("context7"):
    print("‚úÖ MCP context7 server available - loading official docs")
    load_language_docs(language, version)
else:
    print("‚ö†Ô∏è  MCP context7 server unavailable - using cached context")
    load_cached_context(chapter_number)
```

---

## ORCHESTRATED WORKFLOW (What Actually Happens)

When you run `/sp.chapter-writer [N] --language [L]`:

### PHASE 0: Intelligent Context Gathering (Adaptive + MCP-Enhanced)

**Intelligence-Driven Discovery** (not hardcoded questions):

**Step 1: Parse Arguments**
```python
chapter_num = parse_arg(ARGUMENTS, position=0)  # Required
language = parse_arg(ARGUMENTS, "--language") or auto_detect_language(chapter_num)
dry_run = parse_flag(ARGUMENTS, "--dry-run")  # Optional

# Validate inputs
if not chapter_num or chapter_num < 1 or chapter_num > 56:
    error("Chapter number must be between 1 and 56")
if language not in ["python", "typescript", "conceptual"]:
    error(f"Unsupported language: {language}. Use python, typescript, or conceptual.")
```

**Step 2: Read Authoritative Sources**
```bash
# Constitution for audience, philosophy, principles
constitution=$(cat .specify/memory/constitution.md)

# Chapter index for title, part, prerequisites
chapter_data=$(grep "^| $chapter_num |" specs/book/chapter-index.md)
chapter_title=$(echo "$chapter_data" | awk -F'|' '{print $3}' | trim)
chapter_file=$(echo "$chapter_data" | awk -F'|' '{print $4}' | sed 's/`//g' | trim)
part_num=$(determine_part_from_chapter $chapter_num)

# Skills available
skills=$(ls -1 .claude/skills/)

# Context materials (if exist)
context_files=$(find context/ -name "*chapter-$chapter_num*" 2>/dev/null)
```

**Step 3: Load Language Documentation via MCP** (NEW)
```python
if language != "conceptual":
    try:
        # Check MCP server availability
        if mcp_server_available("context7"):
            print(f"üìö Loading {language} documentation via MCP context7...")

            if language == "python":
                version = "3.14"  # Or fetch latest stable
                docs = mcp_fetch_python_docs(version, sections=[
                    "tutorial",
                    "library/stdtypes",
                    "library/functions",
                    f"library/{chapter_focus}"  # e.g., "library/operator" for Ch 15
                ])

            elif language == "typescript":
                version = "5.3"  # Or fetch latest stable
                docs = mcp_fetch_typescript_docs(version, sections=[
                    "handbook",
                    "reference/advanced-types",
                    f"reference/{chapter_focus}"
                ])

            context["language_docs"] = docs
            context["doc_source"] = f"{language.title()} Official Documentation v{version}"
            print(f"‚úÖ Loaded {language} v{version} documentation")

        else:
            print("‚ö†Ô∏è  MCP context7 server unavailable")
            print("    Using cached context materials as fallback")
            context["language_docs"] = load_cached_context(chapter_num, language)
            context["doc_source"] = "Cached context materials"

    except Exception as e:
        print(f"‚ö†Ô∏è  MCP documentation loading failed: {e}")
        print("    Proceeding with cached context")
        context["language_docs"] = load_cached_context(chapter_num, language)
        context["doc_source"] = "Cached context (MCP fallback)"

else:
    # Conceptual chapter - no language docs needed
    context["language_docs"] = None
    context["doc_source"] = "N/A (conceptual chapter)"
```

**Step 4: Derive Chapter Intelligence**
```python
# From constitution (no need to ask user)
audience = "Aspiring/Professional/Founders (graduated complexity)"

# From chapter number (automatic tier assignment)
if 1 <= chapter_num <= 16:
    complexity_tier = "beginner"
    cefr_range = "A1-A2"
    max_concepts = 5
    part_language = "describe intent, explore with AI, validate"
elif 17 <= chapter_num <= 37:
    complexity_tier = "intermediate"
    cefr_range = "A2-B1"
    max_concepts = 7
    part_language = "write specifications, test understanding, apply patterns"
elif 38 <= chapter_num <= 48:
    complexity_tier = "advanced"
    cefr_range = "B1-B2"
    max_concepts = 10
    part_language = "design architecture, define specifications, optimize performance"
elif 49 <= chapter_num <= 56:
    complexity_tier = "professional"
    cefr_range = "B2-C1"
    max_concepts = None  # No artificial limits
    part_language = "production deployment, system design, scalability"

# From chapter index (THE ANCHOR)
part_num = determine_part(chapter_num)
prerequisites = f"Chapters 1-{chapter_num - 1}"
learning_pattern = determine_learning_pattern(part_num)  # AI-Native Learning, SDD, etc.

# Store derived intelligence
chapter_intelligence = {
    "number": chapter_num,
    "title": chapter_title,  # FROM CHAPTER-INDEX.MD (authoritative)
    "part": part_num,
    "language": language,
    "complexity_tier": complexity_tier,
    "cefr_range": cefr_range,
    "max_concepts_per_lesson": max_concepts,
    "prerequisites": prerequisites,
    "audience": audience,
    "learning_pattern": learning_pattern,
    "part_appropriate_language": part_language,
    "available_skills": skills,
    "colearning_required": True,  # Always apply ai-collaborate-teaching
    "documentation_source": context.get("doc_source"),
    "dry_run": dry_run
}
```

**Step 5: Intelligently Determine What to Ask User**
```python
questions = []

# Only ask if genuinely ambiguous or requires human judgment
if context_files:
    questions.append(f"Existing context found for Chapter {chapter_num}. Use it or start fresh?")

if chapter_title_is_broad(chapter_title):  # e.g., "Data Types" could mean many things
    questions.append(f"'{chapter_title}' - which specific aspects should we emphasize?")

if unclear_if_capstone(chapter_num, part_num):
    questions.append("Should students BUILD something hands-on or focus on concepts?")

# Ask only necessary questions (0-3 max, NOT hardcoded)
for q in questions:
    user_input = ask(q)
    chapter_intelligence["user_preferences"][q] = user_input
```

**Step 6: Confirm Intelligence Gathered (DO NOT create branch yet)**

**CRITICAL**: Branch creation happens in Phase 1 AFTER spec.md is created, ensuring branch name matches spec directory name.

**Step 7: Report Intelligence Summary**
```
‚úÖ PHASE 0 COMPLETE: Intelligence Gathered

üìä Chapter Intelligence:
  - Chapter: {number} - {title}
  - Part: {part_num}
  - Language: {language}
  - Complexity: {complexity_tier} (CEFR {cefr_range})
  - Max Concepts/Lesson: {max_concepts}
  - Prerequisites: Chapters 1-{chapter_num - 1}
  - Documentation: {doc_source}
  - CoLearning: ‚úÖ ai-collaborate-teaching skill will be applied
  - Dry Run: {dry_run}

Proceeding to PHASE 1: Specification...
```

---

### PHASE 1: Specification (Automated + Quality Gate)

**THIS PHASE INVOKES `/sp.specify` AUTOMATICALLY WITH FULL CONTEXT INCLUDING MCP DOCS**

1. **Prepare context** (Ruthless filtering + MCP docs + CoLearning principles):
   - Gather chapter intelligence from Phase 0
   - Extract language documentation from MCP (if available)
   - Extract context materials (if available and user approved)
   - Apply ruthless filtering: Skip future chapters, skip advanced variations
   - Embed CoLearning principles (ai-collaborate-teaching skill patterns)
   - Embed learning pattern (AI-Native Learning, SDD, etc.)
   - Embed cognitive load limits (5/7/10 based on tier)

2. **Invoke /sp.specify with full context**:
   ```
   /sp.specify [chapter-slug]

   Write Chapter [N]: [Title] in Part [P] for [Language]

   [Full context: intelligence, CoLearning patterns, MCP docs, teaching patterns, cognitive load rules, filtered materials]
   ```

3. **Wait for /sp.specify completion**:
   - ‚úÖ `specs/part-[P]-chapter-[N]/spec.md` created
   - ‚úÖ CoLearning principles specified
   - ‚úÖ Learning objectives aligned with proficiency levels

4. **Invoke /sp.clarify (Quality Gate)**:
   - Read spec.md
   - Identify underspecified areas
   - Ask up to 5 targeted questions
   - Update spec.md with answers

5. **Create Feature Branch (AFTER spec exists)**:
   ```bash
   # Derive branch name from spec directory (e.g., specs/part-4-chapter-15/ ‚Üí part-4-chapter-15)
   spec_dir_name=$(basename "$(dirname "$FEATURE_SPEC")")
   current_branch=$(git branch --show-current)

   if [[ "$current_branch" == "main" ]]; then
       git checkout -b "$spec_dir_name"
       echo "‚úÖ Created branch: $spec_dir_name"
   elif [[ "$current_branch" == "$spec_dir_name" ]]; then
       echo "‚ÑπÔ∏è  Already on correct branch: $current_branch"
   else
       echo "‚ö†Ô∏è  Warning: Current branch ($current_branch) != spec directory ($spec_dir_name)"
       echo "    Consider switching: git checkout -b $spec_dir_name"
   fi
   ```

6. **Output approval checkpoint**:
   ```
   ‚úÖ PHASE 1 COMPLETE: Specification Created & Clarified

   üìã specs/part-[P]-chapter-[N]/spec.md

   Please review and confirm:
   - ‚úÖ "Spec approved" to proceed to planning
   - üìù Feedback to revise specification
   - ‚ùì Questions for clarification
   ```

6. **Wait for user approval**: Block until user confirms OR provides feedback

---

### PHASE 2: Planning (Automated + ADR Gate)

**THIS PHASE INVOKES `/sp.plan` AUTOMATICALLY WITH COLEARNING CONTEXT**

1. **Prepare context**:
   - Read approved spec.md
   - Add CEFR proficiency levels
   - Add skills proficiency mapping (skills-proficiency-mapper)
   - Add CoLearning structural elements (üí¨üéìüöÄ‚ú®)
   - Add lesson progression (Foundation ‚Üí AI-Assisted ‚Üí Verification balance)
   - Add AI prompts for each lesson
   - Add MCP documentation reference

2. **Invoke /sp.plan with full context**:
   ```
   /sp.plan [chapter-slug]

   [Full context: spec, CEFR levels, CoLearning patterns, lesson structure, MCP docs]
   ```

3. **Wait for /sp.plan completion**:
   - ‚úÖ `specs/part-[P]-chapter-[N]/plan.md` created
   - ‚úÖ Lessons broken down with CoLearning elements specified
   - ‚úÖ Skills proficiency mapped

4. **Invoke /sp.adr (Architectural Decision Gate)**:
   - Detect significant decisions
   - Suggest ADR creation (wait for user consent)

5. **Output approval checkpoint**:
   ```
   ‚úÖ PHASE 2 COMPLETE: Plan Created (+ ADR if applicable)

   üìã specs/part-[P]-chapter-[N]/plan.md

   Please review and confirm:
   - ‚úÖ "Plan approved" to proceed to tasks
   - üìù Feedback to revise plan
   ```

6. **Wait for user approval**

---

### PHASE 3: Tasks (Automated + Analysis Gate)

**THIS PHASE INVOKES `/sp.tasks` AND `/sp.analyze` AUTOMATICALLY**

1. **Invoke /sp.tasks**:
   ```
   /sp.tasks [chapter-slug]

   [Context: spec + plan, acceptance criteria, CoLearning validation steps]
   ```

2. **Invoke /sp.analyze (Cross-Artifact Consistency Gate)**:
   - Validate spec ‚Üî plan ‚Üî tasks alignment
   - Check CoLearning elements present in plan
   - Verify cognitive load limits
   - Report consistency issues

3. **Output approval checkpoint**:
   ```
   ‚úÖ PHASE 3 COMPLETE: Tasks + Analysis

   üìã specs/part-[P]-chapter-[N]/tasks.md
   üìä Analysis report: [findings]

   Please review and confirm:
   - ‚úÖ "Tasks approved" to proceed to implementation
   - üìù Feedback to address issues
   ```

4. **Wait for user approval**

---

### PHASE 4: Implementation (Automated + Technical Review Gate)

**THIS PHASE INVOKES `/sp.implement` WITH EXPLICIT COLEARNING INSTRUCTIONS**

**CRITICAL**: Only execute if NOT dry-run mode

```python
if dry_run:
    print("üèÅ DRY RUN MODE: Stopping at tasks generation")
    print("   To implement: Run without --dry-run flag")
    exit(0)
```

1. **Invoke /sp.implement with CoLearning context**:
   ```
   /sp.implement [chapter-slug]

   CRITICAL INSTRUCTIONS FOR lesson-writer:

   Apply these domain skills IN THIS ORDER:
   1. ai-collaborate-teaching (CoLearning pedagogy THROUGHOUT lesson)
   2. learning-objectives (aligned with CEFR proficiency levels)
   3. concept-scaffolding (graduated complexity)
   4. code-example-generator (if technical chapter)
   5. exercise-designer (deliberate practice)

   CoLearning Structural Elements (MUST appear throughout lesson):
   - üí¨ AI Colearning Prompt: After foundational concepts, encourage AI exploration
   - üéì Instructor Commentary: Emphasize "syntax cheap, semantics gold"
   - üöÄ CoLearning Challenge: Practice specification-driven thinking with AI
   - ‚ú® Teaching Tip: Build AI tool literacy and collaboration patterns

   Tone Requirements:
   - ‚úÖ Conversational (you, your, we)
   - ‚úÖ Exploration-focused (discover, explore, try)
   - ‚úÖ AI partnership (co-teacher, pair-teacher)
   - ‚ùå NOT documentation style
   - ‚ùå NOT reference manual

   Lesson Closure:
   - ‚úÖ ONLY "Try With AI" section at end (4 prompts)
   - ‚ùå NO summaries, checklists, "what's next" after Try With AI

   [Full context: spec, plan, tasks, MCP docs, language standards]
   ```

2. **Wait for lesson-writer completion**:
   - ‚úÖ All lesson files created
   - ‚úÖ CoLearning elements present throughout
   - ‚úÖ "Try With AI" closure only

3. **Invoke technical-reviewer (Quality Gate)**:
   ```
   Validate Chapter [N] with focus on:
   - AI-Native CoLearning pedagogy (üí¨üéìüöÄ‚ú® elements present)
   - Conversational tone (not documentation)
   - Lesson closure (Try With AI ONLY)
   - Technical accuracy (code runs, types correct)
   - Constitution compliance
   ```

4. **Apply critical fixes** (if needed):
   - Re-run technical-reviewer until PASS

5. **Final report**:
   ```
   ‚úÖ CHAPTER [N] COMPLETE & VALIDATED

   üìö Lessons: [count] lessons created
   üìã Validation: PASS
   üéØ CoLearning: ‚úÖ Applied throughout

   Next: Update chapter-index.md
   ```

---

### PHASE 5: Finalization (Update Chapter Index)

**Update chapter-index.md with completion status**

```bash
# Update chapter-index.md
# Mark chapter as ‚úÖ Implemented & Validated
# Add status block with lesson count, validation result, date
```

---

## KEY PRINCIPLES (Always Applied)

### ‚úÖ CoLearning Pedagogy First (ai-collaborate-teaching skill)
- Apply CoLearning structural elements throughout lessons (not just end)
- Conversational tone, exploration-focused, AI partnership emphasized
- 40/40/20 balance (Foundation 40%, AI-Assisted 40%, Verification 20%)
- AI positioned as co-reasoning partner, not autocomplete

### ‚úÖ MCP-Enhanced Intelligence (When Available)
- Load official language documentation via context7 MCP server
- Fallback to cached context if MCP unavailable
- Reference docs for technical accuracy throughout workflow

### ‚úÖ Language-Agnostic Design
- Single command for Python, TypeScript, conceptual chapters
- Language-specific standards applied automatically
- Parameterized via `--language` flag or auto-detection

### ‚úÖ Constitution Compliance
- Graduated Teaching Pattern (Tier 1/2/3)
- Cognitive Load Limits (5/7/10/unlimited based on tier)
- No Forward References (chapter boundaries respected)
- Lesson Closure (Try With AI ONLY)

### ‚úÖ Vertical Intelligence Preserved
- Every phase applies CoLearning principles
- Every phase uses teaching patterns
- Every phase respects chapter boundaries
- Every phase validates against acceptance criteria

---

## EXECUTION INSTRUCTIONS (For Claude Code)

**CRITICAL**: This is an EXECUTABLE orchestration prompt. Claude Code must:
1. Follow this flow exactly, in order
2. Automatically invoke downstream commands WITHOUT asking approval first
3. Pass full context (CoLearning, MCP docs, teaching patterns) to each command
4. Respect approval gates ONLY between phases

### Command Parsing

```python
import sys

args = sys.argv[1:]  # $ARGUMENTS

# Parse chapter number (required)
chapter_num = None
for arg in args:
    if arg.isdigit():
        chapter_num = int(arg)
        break

if not chapter_num:
    error("Usage: /sp.chapter-writer <chapter-number> [--language <lang>] [--dry-run]")

# Parse language (optional, auto-detect if missing)
language = None
for i, arg in enumerate(args):
    if arg == "--language" and i + 1 < len(args):
        language = args[i + 1]

if not language:
    language = auto_detect_language(chapter_num)  # From chapter-index.md

# Parse dry-run flag
dry_run = "--dry-run" in args

print(f"üìñ Chapter {chapter_num}: {language} chapter")
print(f"   Dry run: {dry_run}")
```

### Auto-Detection Logic

```python
def auto_detect_language(chapter_num: int) -> str:
    """Detect language from chapter number ranges."""
    # Python chapters
    python_chapters = list(range(12, 30)) + list(range(41, 46))

    # TypeScript chapters
    typescript_chapters = list(range(38, 41)) + list(range(46, 49))

    # Conceptual chapters (everything else)
    if chapter_num in python_chapters:
        return "python"
    elif chapter_num in typescript_chapters:
        return "typescript"
    else:
        return "conceptual"
```

### MCP Integration Pattern

```bash
# Check MCP server availability
check_mcp_context7() {
    # Attempt to list MCP servers
    if command -v mcp &> /dev/null; then
        if mcp list 2>/dev/null | grep -q "context7"; then
            return 0  # Available
        fi
    fi
    return 1  # Not available
}

# Load language documentation
load_language_docs() {
    local language=$1
    local version=$2

    if check_mcp_context7; then
        echo "üìö Loading $language v$version documentation via MCP..."

        # Example: Fetch Python docs (actual MCP API call)
        # This is pseudo-code - actual implementation depends on MCP CLI/API
        mcp fetch context7 python-docs --version $version \
            --sections tutorial,library/stdtypes,library/functions \
            > /tmp/chapter_docs_$language.json

        echo "‚úÖ Documentation loaded"
        return 0
    else
        echo "‚ö†Ô∏è  MCP context7 unavailable - using cached context"
        return 1
    fi
}
```

---

## VALIDATION CHECKLIST

Before releasing this command, verify:

**Phase 0 Checks**:
- [ ] MCP context7 server detection works
- [ ] Language documentation loading succeeds (or fails gracefully)
- [ ] Auto-detection logic correct for all chapter ranges
- [ ] Chapter intelligence derived correctly

**Phase 1-3 Checks**:
- [ ] /sp.specify receives CoLearning context
- [ ] /sp.plan includes CoLearning structural elements
- [ ] /sp.tasks includes CoLearning validation steps

**Phase 4 Checks**:
- [ ] lesson-writer receives explicit CoLearning instructions
- [ ] Generated lessons have üí¨üéìüöÄ‚ú® elements throughout
- [ ] Generated lessons have conversational tone
- [ ] Generated lessons end with "Try With AI" ONLY
- [ ] technical-reviewer validates CoLearning pedagogy

**Cross-Cutting Checks**:
- [ ] Python chapters use Python 3.14+ standards
- [ ] TypeScript chapters use TypeScript 5.3+ standards
- [ ] Conceptual chapters have no code requirements
- [ ] Cognitive load limits enforced (5/7/10 based on tier)
- [ ] Constitution compliance verified

---

## EXAMPLES

### Example 1: Python Chapter (Standard)

```bash
/sp.chapter-writer 15 --language python
```

**Expected Workflow**:
1. Phase 0: Load Python 3.14 docs via MCP, detect "beginner" tier
2. Phase 1: Create spec with CoLearning principles
3. Phase 2: Plan 5 lessons with üí¨üéìüöÄ‚ú® elements
4. Phase 3: Generate 99 tasks with CoLearning validation
5. Phase 4: lesson-writer creates lessons with conversational tone
6. Phase 5: Update chapter-index.md

---

### Example 2: TypeScript Chapter (Dry Run)

```bash
/sp.chapter-writer 38 --language typescript --dry-run
```

**Expected Workflow**:
1. Phase 0: Load TypeScript 5.3 docs via MCP
2. Phase 1-3: Generate spec, plan, tasks
3. **STOP** (dry run mode - no implementation)
4. User reviews artifacts, then runs without --dry-run to implement

---

### Example 3: Conceptual Chapter (Auto-Detect)

```bash
/sp.chapter-writer 5
```

**Expected Workflow**:
1. Phase 0: Auto-detect "conceptual" (no language docs needed)
2. Phase 1-3: Generate spec, plan, tasks (no code requirements)
3. Phase 4: lesson-writer creates narrative content with CoLearning prompts
4. Lessons have reflection prompts, not coding exercises

---

## CRITICAL SUCCESS FACTORS

1. **MCP Integration**: Official docs loaded when available, graceful fallback when not
2. **CoLearning Throughout**: ai-collaborate-teaching applied to every lesson, not just end
3. **Conversational Tone**: Lessons feel like AI co-teacher dialogue, not reference manual
4. **Language-Agnostic**: Single command handles Python, TypeScript, conceptual
5. **Constitution Compliant**: All 17 principles enforced automatically
6. **Automatic Invocation**: Commands chain automatically with approval gates between phases
7. **Context Preservation**: Full intelligence flows through all phases

---

## TROUBLESHOOTING

### MCP Context7 Not Available

**Symptom**: "‚ö†Ô∏è MCP context7 server unavailable"

**Impact**: Fallback to cached context materials

**Resolution**:
- Verify MCP server configuration in Claude Desktop settings
- Check context7 server is installed and running
- Use `--dry-run` to test without docs, review artifacts manually

### Lessons Missing CoLearning Elements

**Symptom**: Generated lessons don't have üí¨üéìüöÄ‚ú® sections

**Root Cause**: lesson-writer didn't receive explicit CoLearning instructions

**Resolution**:
- Verify Phase 4 passes explicit instructions (see "CRITICAL INSTRUCTIONS FOR lesson-writer" in Phase 4)
- Check ai-collaborate-teaching skill has `required_for: ["lesson-writer"]` metadata
- Re-run technical-reviewer to validate CoLearning compliance

### Documentation Style Instead of Conversational

**Symptom**: Lessons read like API reference, not teaching dialogue

**Root Cause**: Tone requirements not emphasized enough to lesson-writer

**Resolution**:
- Emphasize tone requirements in Phase 4 invocation
- Include example CoLearning sections in context
- Provide feedback to lesson-writer: "Rewrite in conversational tone"

---

## REFERENCES

- **Constitution**: `.specify/memory/constitution.md` (Principles 1, 12, 13 especially)
- **Chapter Index**: `specs/book/chapter-index.md` (THE ANCHOR for chapter titles)
- **ai-collaborate-teaching Skill**: `.claude/skills/ai-collaborate-teaching/SKILL.md`
- **Output Style**: `.claude/output-styles/lesson.md`
- **MCP Documentation**: [MCP Protocol Spec](https://modelcontextprotocol.io/docs)

---

## ONE COMMAND. FULL INTELLIGENCE. COLEARNING BUILT-IN. MCP-ENHANCED.

Run `/sp.chapter-writer [N] --language [L]` and the system executes this opinionated workflow:

**PHASE 0**: Intelligent Context Discovery + MCP Documentation Loading
**PHASE 1**: Specification + Clarification (CoLearning principles embedded)
**PHASE 2**: Planning + ADR (CoLearning structural elements defined)
**PHASE 3**: Tasks + Analysis (CoLearning validation steps included)
**PHASE 4**: Implementation + Technical Review (lesson-writer applies ai-collaborate-teaching)
**PHASE 5**: Finalization (chapter-index.md updated)

**Result**: High-quality, AI-Native CoLearning chapters with built-in pedagogy, official documentation references, and constitutional compliance.

---

**Note**: For PHR (Prompt History Record) creation after command completion, see constitution for instructions.
